{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic neural network is called a **multi-layered perception**. It is a neural network where the neurons in each layer are connected to **all** the neurons in the next layer. For this reason we call it a **dense** network.\n",
    "\n",
    "In this notebook we use `numpy` to manually create and train a neural network. We do this mostly so we can build some intuition around what happens behind the scene when we train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we use is manually created. We want to have a very small data set so that we can look at intermediate results as we build our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(np.arange(0.1, 0.7, 0.1))\n",
    "X1 = np.exp(X1 * 1.1 + 0.75)\n",
    "X2 = np.array(np.arange(0.6, 1.2, 0.1))\n",
    "X2 = np.exp(X2 * 0.4 + 0.75)\n",
    "X3 = np.random.random(6)\n",
    "X3 = np.exp(X3 * 0.4 + 0.75)\n",
    "\n",
    "X_train = np.array([X1, X2, X3]).T\n",
    "y_train = (X_train[:,:1] > 3).all(axis = 1).reshape(6, 1)\n",
    "\n",
    "print(np.hstack([X_train, y_train]))\n",
    "del X1, X2, X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a logistic regression\n",
    "\n",
    "Before we train a neural network, it might be worthwhile asking what we would do if we had to solve this using the tools we already have at our disposal. Since our target is binary, using a `LogisticRegression` is one easy option. So let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod = LogisticRegression()\n",
    "logmod.fit(X_train, y_train.ravel())\n",
    "y_hat = logmod.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model. Usually we would evaluate the model on the training data. We'll worry about test data later. For now that's besides the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with logistic regression, we can train a model that seems to quickly find the decision boundary. How does logistic regression make its prediction? It uses the following formula to get raw predictions .\n",
    "\n",
    "$$\\text{raw\\_predictions} = b_0 + b_1x_1 + b_2x_2 + b_3x_3$$\n",
    "\n",
    "In previous lectures, we referred to $b_0$, $b_1$ and $b_2$ as the model's **parameters**: $b_0$ is called the **intercept** and $b_1$, $b_2$ and $b_3$ are called **coefficients**. These raw predictions represent our confidence about how likely it is that any row of the data would belong to the positive class. But the scale of these raw predictions are somewhat arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model intercept (bias): \")\n",
    "print(logmod.intercept_)\n",
    "print(\"Model coefficients (weights): \")\n",
    "print(logmod.coef_.T)\n",
    "\n",
    "pred = logmod.intercept_ + np.dot(X_train, logmod.coef_.T)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of an **activation function**. Here we use the **sigmoid** activation function, also called the **logistic** activation function, given by $\\sigma(z) = \\frac{1}{1+e^-z}$. It forces the activations to be between 0 and 1. Before passing the input to this function, use `np.clip` to trim it between -500 and 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the raw predictions and pass them to a **sigmoid** function and get predictions that are rescaled to be between 0 and 1. We interpret these scaled predictions as the probability that a given row belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.hstack([sigmoid(pred), y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, the above is what we obtain when we run the `predict_proba` method of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmod.predict_proba(X_train) # the second column shows the probability of Y = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when the prediction is below 0.50 the labels are 0 and otherwise the labels are 1. The reason we started with `LogisticRegression` is because the way that it trains is very similar to a neural network. In fact, a logistic regression model is a neural network with **no hidden layer**. So let's now manually create our neural network and see how we can get a result similar to what `LogisticRegression` obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Let's return to our prediction equation:\n",
    "\n",
    "$$\\text{raw\\_predictions} = b_0 + b_1x_1 + b_2x_2 + b_3x_3$$\n",
    "\n",
    "In neural networks, we prefer to use the word **bias** for the intercept and **weights** for the coefficients. We saw how logistic regression found its parameters. Now we want to see how a neural network finds its parameters? It starts with some random values for them. We call this **random initialization**. We usually generate numbers that are random but close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(dim1, dim2 = 1, std = 1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1, dim2]) * std)\n",
    "    else:\n",
    "        return(np.zeros([dim1, dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have values for the parameters, we can now run a **forward pass**, which ultimately ends in **predictions**. Of course, because we randomly initialized our parameters, the first time around the predictions are as good as coin tosses.\n",
    "\n",
    "Note that our forward pass consists of a matrix multiplication, for which we use `np.dot`. The forward pass takes the input data and multiplies it by weights and adds the bias, the result of which is called a **weighted sum**, called `Z1` below. It then applies the **activation function** to the weighted sum, we get the **activations**, called `A1` here. \n",
    "\n",
    "In this example, we don't have any hidden layers, so our forward pass will take us directly from the data to the predictions. But if we had hidden layers, we would run this same calculation once for each hidden layer, finally finishing with the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W1, bias, X):\n",
    "    Z1 = np.dot(X, W1) + bias\n",
    "    A1 = sigmoid(Z1)\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, input_cols = X_train.shape\n",
    "_, output_cols = y_train.shape\n",
    "\n",
    "weights = init_parameters(input_cols, output_cols)\n",
    "bias = init_parameters(output_cols)\n",
    "\n",
    "print(\"Checking dimensions: {} * {} + {}\".format(X_train.shape, weights.shape, bias.shape))\n",
    "\n",
    "pred = forward(weights, bias, X_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the forward pass, we now have a prediction. Our next question is how can we improve our prediction? The answer is that we need to calculate our error (called **loss**) and from that derive the **derivative of loss w.r.t. weights and biases**. In multivariate calculus, this derivative is called the **gradient**.\n",
    "\n",
    "In previous lectures, we learned that for classification model, we can measure the error by looking at **accuracy** (or precision and recall for imbalanced data). However, as it turns out these metrics are not going to work well here, because in order to get derivatives in calculus we need **continuous functions**, and accuracy, precision or recall are not continuous functions of our weights and biases. Another problem is that these metrics are obtained **after** we define our threshold, and can change if we change our threshold. So we need something else.\n",
    "\n",
    "One loss function that works well with classification is the **cross-entropy loss**. For binary classification, cross-entropy for the $i$th data point is defined as $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i)$, where $y_i$ is our binary target, and $\\hat y_i$ is the prediction (activation at the output layer) at row $i$. Cross-entropy for the whole data is just the average of the cross-entropies at each row.\n",
    "\n",
    "While we don't show the derivation here, once we define our loss function, we can get the derivative of loss w.r.t. the activations `A1`, and then (using the chain rule) get the derivative of loss w.r.t. to `Z1`, and finally w.r.t. weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(A1, W1, bias, X, Y):\n",
    "\n",
    "    m = np.shape(X)[0] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    loss = Y * np.log(A1) + (1 - Y)*np.log(1 - A1)           # loss at each row\n",
    "    cost = (-1/m) * np.sum(loss)                             # loss across all rows\n",
    "    dZ1 = A1 - Y                                             # derivative of loss wrt Z1\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)                           # derivative of loss wrt weights\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 0, keepdims = True) # derivative of loss wrt bias\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"dB1\": dBias}                       # updated weights and biases\n",
    "    \n",
    "    return(grads, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's once again test the output to make sure it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradients, _ = backward(pred, weights, bias, X_train, y_train)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all we need to start running our optimization routine: a simple implementation of **gradient descent**. This consists of iteratively running forward propagation to get predictions, the backpropagation to get the gradient of loss w.r.t. weights and biases, and finally moving weights and biases in the direction of their gradient. For the latter, we control the size of the step using a constant we call the **learning rate**. As we do this, we record loss at each iteration so that we can plot it and make sure that loss is decreasing at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs, learning_rate, X, Y):\n",
    "    \n",
    "    m, input_cols = X.shape\n",
    "    \n",
    "    W1 = init_parameters(input_cols, output_cols)\n",
    "    B1 = init_parameters(output_cols)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan     # place-holder of keeping track of loss\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = forward(W1, B1, X)                   # get activations in final layer\n",
    "        grads, cost = backward(A1, W1, B1, X, Y)  # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]      # update weights\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]      # update bias\n",
    "        \n",
    "        loss_array[i] = cost                      # record loss for current iteration\n",
    "        \n",
    "        parameter = {\"W1\": W1, \"B1\": B1}          # record parameters for current iteration\n",
    "    \n",
    "    return(parameter, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Let's now run our gradient descent function for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs, learning_rate, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After letting the network train for many iterations, these are the final parameters we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params['B1'][0])\n",
    "print(params['W1'].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the parameters we got when we trained a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logmod.intercept_)\n",
    "print(logmod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the parameters don't necessarily look similar. There can be a lot of reasons for that. Because our data is close to linearly separable, there are a lot of possible solutions. There could also be differences between the `sklearn` logistic regression and our implementation of neural networks. So instead of comparing the parameters, let's compare the predictions: we can put the predictions we get from using the parameters for the neural network and logistic regression side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_nn = params['B1'] + np.dot(X_train, params['W1'])\n",
    "y_pred_logit = logmod.intercept_ + np.dot(X_train, logmod.coef_.T)\n",
    "\n",
    "np.hstack([y_pred_logit, Y_pred_nn, y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these are raw predictions. So it might be best to pass these to a sigmoid function to turn them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([sigmoid(y_pred_logit), sigmoid(Y_pred_nn), y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in either case if we use 0.50 as the cut-off both models predict correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Run the neural network for 10K iterations instead of 1000 and look at the predictions.\n",
    "- Run the neural network for 100K iterations instead of 1000 and look at the predictions.\n",
    "- Do you see a trend?\n",
    "- Return to where you run `run_grad_desc` and prior to running run the following code: `y_train = np.hstack([y_train, ~y_train])`. Careful! This will break the earlier logistic regression code. Train the network and look at the results that follow. Can you explain what happened? This result has important consequences for our earlier claim that you can do multi-class classification with neural networks using a single model (without resorting to **one-vs-rest** or **one-vs-one** models).\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show how loss drops iteration over iteration that we train this. This means that in a real-world scenario, if we let training continue indefinitely, eventually we will reach a point where we begin over-fitting to the training data. So it's important to have test data set aside that we use for knowing when that happens so we can stop training. This is called **early stopping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = loss_array[::10]); # we only plot every 50th point so plot renders fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we saw how a neural network works. Of course a real neural network would have at least one hidden layer, but hidden layers only add the amount of computation we have to do. The principle stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load the wine.csv data and prepare the data for analysis: Split the data into training and testing and normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Data:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0       0.237288          0.106667     0.284553        0.108896   0.036545   \n",
      "1       0.381356          0.133333     0.390244        0.023006   0.139535   \n",
      "2       0.330508          0.423333     0.008130        0.023006   0.091362   \n",
      "3       0.118644          0.193333     0.268293        0.009202   0.031561   \n",
      "4       0.237288          0.120000     0.455285        0.226994   0.073090   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
      "0             0.240550              0.381657  0.128976  0.354331   0.084270   \n",
      "1             0.034364              0.017751  0.134374  0.409449   0.224719   \n",
      "2             0.206186              0.109467  0.127241  0.527559   0.196629   \n",
      "3             0.082474              0.221893  0.059572  0.496063   0.089888   \n",
      "4             0.213058              0.399408  0.211876  0.291339   0.151685   \n",
      "\n",
      "    alcohol   quality  \n",
      "0  0.403226  0.333333  \n",
      "1  0.709677  0.666667  \n",
      "2  0.612903  0.500000  \n",
      "3  0.596774  0.500000  \n",
      "4  0.209677  0.333333  \n",
      "\n",
      "Normalized Testing Data:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0       0.271186          0.060000     0.601626        0.187117   0.059801   \n",
      "1       0.330508          0.373333     0.170732        0.024540   0.112957   \n",
      "2       0.254237          0.206667     0.276423        0.104294   0.018272   \n",
      "3       0.211864          0.133333     0.382114        0.162577   0.051495   \n",
      "4       0.305085          0.180000     0.162602        0.203988   0.074751   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
      "0             0.158076              0.355030  0.136688  0.409449   0.089888   \n",
      "1             0.213058              0.375740  0.163678  0.417323   0.129213   \n",
      "2             0.254296              0.375740  0.096588  0.346457   0.123596   \n",
      "3             0.412371              0.523669  0.169848  0.299213   0.162921   \n",
      "4             0.426117              0.659763  0.226913  0.291339   0.157303   \n",
      "\n",
      "    alcohol   quality  \n",
      "0  0.677419  0.833333  \n",
      "1  0.306452  0.333333  \n",
      "2  0.645161  0.666667  \n",
      "3  0.241935  0.500000  \n",
      "4  0.145161  0.500000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Convert the normalized arrays back to DataFrames for better readability and saving to CSV\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized, columns=X_test.columns)\n",
    "\n",
    "# Save the processed data to CSV files\n",
    "X_train_normalized.to_csv('X_train_normalized.csv', index=False)\n",
    "X_test_normalized.to_csv('X_test_normalized.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the normalized training and testing sets\n",
    "print('Normalized Training Data:')\n",
    "print(X_train_normalized.head())\n",
    "print('\\nNormalized Testing Data:')\n",
    "print(X_test_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1243327758.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Train a logistic regression classifier to predict the type of wine (red vs. white). Report the accuracy of the model.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Train a logistic regression classifier to predict the type of wine (red vs. white). Report the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the pre-processed data from the CSV files\n",
    "X_train_normalized = pd.read_csv('X_train_normalized.csv')\n",
    "X_test_normalized = pd.read_csv('X_test_normalized.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "log_reg_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "# Train the model using the normalized training data\n",
    "log_reg_model.fit(X_train_normalized, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the normalized testing data\n",
    "y_pred = log_reg_model.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train a multi-layer feed-forward neural network to predict the type of wine. Your network should have one hidden layer. You are free to choose how many neurons you want in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model Accuracy: 0.9954\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the pre-processed data\n",
    "X_train_normalized = pd.read_csv('X_train_normalized.csv')\n",
    "X_test_normalized = pd.read_csv('X_test_normalized.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "\n",
    "# Initialize the Multi-layer Perceptron (MLP) Classifier with two hidden layers\n",
    "# For example, two layers with 100 and 50 neurons respectively\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp_classifier.fit(X_train_normalized, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = mlp_classifier.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Neural Network Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tune your neural network by trying different values for the learning rate and the number of neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'hidden_layer_sizes': (50, 25), 'learning_rate_init': 0.001}\n",
      "Accuracy of the best model on the test set: 0.9923\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the pre-processed data\n",
    "X_train_normalized = pd.read_csv('X_train_normalized.csv')\n",
    "X_test_normalized = pd.read_csv('X_test_normalized.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Define the model to be tuned\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 25), (100, 50), (100, 100)],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the model and parameter grid\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator (model)\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best = best_model.predict(X_test_normalized)\n",
    "\n",
    "# Calculate the accuracy of the best model on the test set\n",
    "best_model_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Accuracy of the best model on the test set: {best_model_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Report the accuracy of the best model you obtained in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (75361124.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    hidden layer sizes: (50, 25)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "hidden layer sizes: (50, 25)\n",
    "learning rate init: 0.001\n",
    "\n",
    "# The accuracy of the tuned neural network model on the test set is 99.23%. \n",
    "# This result is only a minor change from the initial model's accuracy, \n",
    "# showing that the default parameters were already quite effective for this dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd5b1060645c161064a32399f3815b62036dca35fd6bfa9efb7b0f1d11b6ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
